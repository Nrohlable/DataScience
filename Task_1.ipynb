{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RvLczslFW-mC"
   },
   "outputs": [],
   "source": [
    "#To read csv files in Tabular format we would use pandas\n",
    "import pandas as pd\n",
    "data = pd.read_csv('/content/drive/MyDrive/SpendData.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "qhhQPJcOXHdp",
    "outputId": "6a39ae3c-f86f-4a71-ae61-013943ffa2a9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>month</th>\n",
       "      <th>var8</th>\n",
       "      <th>var6</th>\n",
       "      <th>a.1</th>\n",
       "      <th>a.2</th>\n",
       "      <th>a.3</th>\n",
       "      <th>a.4</th>\n",
       "      <th>var5</th>\n",
       "      <th>b.5</th>\n",
       "      <th>b.6</th>\n",
       "      <th>b.7</th>\n",
       "      <th>b.8</th>\n",
       "      <th>b.9</th>\n",
       "      <th>b.10</th>\n",
       "      <th>b.11</th>\n",
       "      <th>b.12</th>\n",
       "      <th>b.13</th>\n",
       "      <th>b.14</th>\n",
       "      <th>b.15</th>\n",
       "      <th>b.16</th>\n",
       "      <th>b.17</th>\n",
       "      <th>b.18</th>\n",
       "      <th>b.19</th>\n",
       "      <th>b.20</th>\n",
       "      <th>b.21</th>\n",
       "      <th>b.22</th>\n",
       "      <th>pov6</th>\n",
       "      <th>b.23</th>\n",
       "      <th>b.24</th>\n",
       "      <th>b.25</th>\n",
       "      <th>b.26</th>\n",
       "      <th>b.27</th>\n",
       "      <th>b.28</th>\n",
       "      <th>c.29</th>\n",
       "      <th>c.30</th>\n",
       "      <th>var7</th>\n",
       "      <th>c.31</th>\n",
       "      <th>c.32</th>\n",
       "      <th>c.33</th>\n",
       "      <th>...</th>\n",
       "      <th>c.246</th>\n",
       "      <th>c.247</th>\n",
       "      <th>c.248</th>\n",
       "      <th>c.249</th>\n",
       "      <th>c.250</th>\n",
       "      <th>c.251</th>\n",
       "      <th>c.252</th>\n",
       "      <th>c.253</th>\n",
       "      <th>c.254</th>\n",
       "      <th>c.255</th>\n",
       "      <th>c.256</th>\n",
       "      <th>c.257</th>\n",
       "      <th>c.258</th>\n",
       "      <th>c.259</th>\n",
       "      <th>c.260</th>\n",
       "      <th>c.261</th>\n",
       "      <th>c.262</th>\n",
       "      <th>c.263</th>\n",
       "      <th>c.264</th>\n",
       "      <th>c.265</th>\n",
       "      <th>c.266</th>\n",
       "      <th>c.267</th>\n",
       "      <th>c.268</th>\n",
       "      <th>c.269</th>\n",
       "      <th>c.270</th>\n",
       "      <th>c.271</th>\n",
       "      <th>c.272</th>\n",
       "      <th>c.273</th>\n",
       "      <th>c.274</th>\n",
       "      <th>c.275</th>\n",
       "      <th>c.276</th>\n",
       "      <th>c.277</th>\n",
       "      <th>c.278</th>\n",
       "      <th>c.279</th>\n",
       "      <th>c.280</th>\n",
       "      <th>c.281</th>\n",
       "      <th>c.282</th>\n",
       "      <th>c.283</th>\n",
       "      <th>f.284</th>\n",
       "      <th>t.158</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>50</td>\n",
       "      <td>50.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>93.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>93.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>57</td>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>102.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>93.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 301 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  month  var8  var6  a.1  ...  c.281  c.282  c.283  f.284  t.158\n",
       "0           1      1   2.0   NaN    1  ...      0      0      0    5.0    NaN\n",
       "1           2      1   2.0   NaN    1  ...      0      1      0    NaN    NaN\n",
       "2           3      1   2.0   NaN    1  ...      0      0      0    3.0    NaN\n",
       "3           4      1   2.0   NaN    1  ...      0      1      0    5.0    NaN\n",
       "4           5      1   2.0   NaN    1  ...      0      0      0    5.0    NaN\n",
       "\n",
       "[5 rows x 301 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YtiCHJWwXjCC",
    "outputId": "61d445f2-792e-4249-e4bb-e61c5a57195a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    14197\n",
       "2     3246\n",
       "3      335\n",
       "6      285\n",
       "4      183\n",
       "5      133\n",
       "Name: pov6, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#As per the Problem statement the Column/Feature 'pov6' has the respondents divided into 6 categories \n",
    "#Here we are analazing about the class imbalance for this Multi-classs Porblem\n",
    "data['pov6'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1I3xtvirqI3k"
   },
   "source": [
    "The above output shows that there is a huge Class imbalance if we compare group 1 with group 5 , group 4 , group 6 and group 3 . This class imbalance have to keep in mind while Training the data.\n",
    "</br>\n",
    "\n",
    "**Upsampling:** We will try to train the same data 1st without Upsampling by keep an eye on Metric and Model used for this data and if it doesn't work then we would try to work it out with upsampling.\n",
    "We could use various modules for this task such as sklearn's Resample , Imblearn's Smotes and etc.\n",
    "</br>\n",
    "\n",
    "\n",
    "**Downsampling:** We shuold not use downsampling here since if we downsample the entire dataset to number of samples present in any of the group btw 3 to 6 , then we would lose a good amount of valuable data \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VR_ND2W7spAG",
    "outputId": "b24ff14a-4c65-4853-e41b-fd4ab12767d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Number of NAN values in Dataset:  809220\n"
     ]
    }
   ],
   "source": [
    "#Getting total number of null values in the Dataset\n",
    "print('Total Number of NAN values in Dataset: ',data.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdoMXSfxs7c4"
   },
   "source": [
    "It seems the given Dataset contains a lot of Null values which needs to be fixed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uUlkfnLceq3-",
    "outputId": "ac72df84-5817-4762-c350-13275f573f12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "var8 Column/Feature total number of NAN Values: 4009\n",
      "var6 Column/Feature total number of NAN Values: 16480\n",
      "b.6 Column/Feature total number of NAN Values: 8079\n",
      "b.8 Column/Feature total number of NAN Values: 1773\n",
      "b.10 Column/Feature total number of NAN Values: 16007\n",
      "b.11 Column/Feature total number of NAN Values: 16007\n",
      "b.13 Column/Feature total number of NAN Values: 11683\n",
      "b.14 Column/Feature total number of NAN Values: 11920\n",
      "b.20 Column/Feature total number of NAN Values: 14234\n",
      "b.21 Column/Feature total number of NAN Values: 15098\n",
      "c.31 Column/Feature total number of NAN Values: 15225\n",
      "c.36 Column/Feature total number of NAN Values: 18148\n",
      "c.37 Column/Feature total number of NAN Values: 18372\n",
      "c.39 Column/Feature total number of NAN Values: 5359\n",
      "c.48 Column/Feature total number of NAN Values: 4475\n",
      "c.60 Column/Feature total number of NAN Values: 7984\n",
      "c.61 Column/Feature total number of NAN Values: 16770\n",
      "c.67 Column/Feature total number of NAN Values: 17436\n",
      "c.77 Column/Feature total number of NAN Values: 78\n",
      "c.92 Column/Feature total number of NAN Values: 3090\n",
      "e.101 Column/Feature total number of NAN Values: 4241\n",
      "f.105 Column/Feature total number of NAN Values: 16159\n",
      "f.106 Column/Feature total number of NAN Values: 16160\n",
      "f.107 Column/Feature total number of NAN Values: 16378\n",
      "f.108 Column/Feature total number of NAN Values: 17394\n",
      "f.109 Column/Feature total number of NAN Values: 16542\n",
      "f.110 Column/Feature total number of NAN Values: 17709\n",
      "f.111 Column/Feature total number of NAN Values: 17450\n",
      "f.112 Column/Feature total number of NAN Values: 15825\n",
      "f.113 Column/Feature total number of NAN Values: 14983\n",
      "f.114 Column/Feature total number of NAN Values: 17435\n",
      "f.115 Column/Feature total number of NAN Values: 17878\n",
      "f.116 Column/Feature total number of NAN Values: 17745\n",
      "f.117 Column/Feature total number of NAN Values: 17985\n",
      "f.118 Column/Feature total number of NAN Values: 17944\n",
      "f.119 Column/Feature total number of NAN Values: 17821\n",
      "f.120 Column/Feature total number of NAN Values: 18042\n",
      "f.121 Column/Feature total number of NAN Values: 18128\n",
      "f.122 Column/Feature total number of NAN Values: 17952\n",
      "f.123 Column/Feature total number of NAN Values: 17874\n",
      "var4 Column/Feature total number of NAN Values: 3090\n",
      "c.125 Column/Feature total number of NAN Values: 77\n",
      "c.135 Column/Feature total number of NAN Values: 67\n",
      "c.140 Column/Feature total number of NAN Values: 4\n",
      "c.141 Column/Feature total number of NAN Values: 3\n",
      "c.145 Column/Feature total number of NAN Values: 1\n",
      "c.147 Column/Feature total number of NAN Values: 1\n",
      "c.148 Column/Feature total number of NAN Values: 1\n",
      "t7.149 Column/Feature total number of NAN Values: 67\n",
      "t7.153 Column/Feature total number of NAN Values: 4\n",
      "t7.154 Column/Feature total number of NAN Values: 2\n",
      "t7.157 Column/Feature total number of NAN Values: 3\n",
      "t7.158 Column/Feature total number of NAN Values: 76\n",
      "a.183 Column/Feature total number of NAN Values: 5360\n",
      "a.184 Column/Feature total number of NAN Values: 18148\n",
      "a.185 Column/Feature total number of NAN Values: 18372\n",
      "c.189 Column/Feature total number of NAN Values: 298\n",
      "c.190 Column/Feature total number of NAN Values: 5361\n",
      "var9 Column/Feature total number of NAN Values: 237\n",
      "b.195 Column/Feature total number of NAN Values: 14605\n",
      "c.214 Column/Feature total number of NAN Values: 5356\n",
      "c.215 Column/Feature total number of NAN Values: 18148\n",
      "c.216 Column/Feature total number of NAN Values: 18372\n",
      "c.217 Column/Feature total number of NAN Values: 18264\n",
      "c.218 Column/Feature total number of NAN Values: 18088\n",
      "c.219 Column/Feature total number of NAN Values: 18297\n",
      "c.220 Column/Feature total number of NAN Values: 18374\n",
      "c.221 Column/Feature total number of NAN Values: 18350\n",
      "c.222 Column/Feature total number of NAN Values: 15012\n",
      "c.223 Column/Feature total number of NAN Values: 18292\n",
      "c.250 Column/Feature total number of NAN Values: 221\n",
      "f.284 Column/Feature total number of NAN Values: 4418\n",
      "t.158 Column/Feature total number of NAN Values: 18379\n"
     ]
    }
   ],
   "source": [
    "#Getting all the columns which have null values along with the number of null values that they have in the entire column\n",
    "\n",
    "data_columns = list(data.columns)\n",
    "null_columns = []\n",
    "non_null_columns = []\n",
    "null_values = []\n",
    "count = 0\n",
    "for i in range(len(data_columns)):\n",
    "  if data[data_columns[i]].isnull().sum() == 0:\n",
    "    non_null_columns.append(data_columns[i])\n",
    "    pass\n",
    "  else:\n",
    "    print(data_columns[i], end = \" \")\n",
    "    print('Column/Feature total number of NAN Values:', end= ' ')\n",
    "    print(data[data_columns[i]].isnull().sum())\n",
    "    count += 1\n",
    "    null_columns.append(data_columns[i])\n",
    "    null_values.append(data[data_columns[i]].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PabeHDStgKa5",
    "outputId": "05eda9ec-0bad-41c4-f37e-1c5db6d511af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of rows with NAN values:  73\n"
     ]
    }
   ],
   "source": [
    "print('Total number of rows with NAN values: ', count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JB2gVp_bgXRr",
    "outputId": "5440e7de-8158-4e8e-c187-4fa537ed3369"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Columns/Features Rejected due to Threshold value: 60\n",
      "Number of Columns/Features Selected to be in Dataset as per Threshold value: 13\n",
      "Shape of Data with removing columns with high amount of null_values  (18379, 241)\n"
     ]
    }
   ],
   "source": [
    "#Setting a thresold for NAN Values\n",
    "#Here we are setting the threshold to 99 and rejecting all other columns that contains number of null values in 3 digits \n",
    "\n",
    "total_rows = len(data)\n",
    "threshold_value = 99\n",
    "rejected_columns = []\n",
    "appending_columns = []\n",
    "for i in range(len(null_values)):\n",
    "  if null_values[i] >= threshold_value:\n",
    "    rejected_columns.append(null_columns[i])\n",
    "  else:\n",
    "    appending_columns.append(null_columns[i])\n",
    "\n",
    "\n",
    "print('Number of Columns/Features Rejected due to Threshold value:', len(rejected_columns))\n",
    "print('Number of Columns/Features Selected to be in Dataset as per Threshold value:', len(appending_columns))\n",
    "\n",
    "data.drop(columns= rejected_columns , inplace= True)\n",
    "print('Shape of Data with removing columns with high amount of null_values ',data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVvUUyA00wix"
   },
   "source": [
    "Now we need to now fill all the null values with somw kind of logic in order to remove null values entirely from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 307
    },
    "id": "9jF0PID4xnrO",
    "outputId": "769b7124-204e-4252-c23c-b27f6fa09f65"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c.77</th>\n",
       "      <th>c.125</th>\n",
       "      <th>c.135</th>\n",
       "      <th>c.140</th>\n",
       "      <th>c.141</th>\n",
       "      <th>c.145</th>\n",
       "      <th>c.147</th>\n",
       "      <th>c.148</th>\n",
       "      <th>t7.149</th>\n",
       "      <th>t7.153</th>\n",
       "      <th>t7.154</th>\n",
       "      <th>t7.157</th>\n",
       "      <th>t7.158</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>18301.000000</td>\n",
       "      <td>18302.000000</td>\n",
       "      <td>18312.000000</td>\n",
       "      <td>18375.000000</td>\n",
       "      <td>18376.000000</td>\n",
       "      <td>18378.000000</td>\n",
       "      <td>18378.000000</td>\n",
       "      <td>18378.000000</td>\n",
       "      <td>18312.000000</td>\n",
       "      <td>18375.000000</td>\n",
       "      <td>18377.000000</td>\n",
       "      <td>18376.000000</td>\n",
       "      <td>18303.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>442.001570</td>\n",
       "      <td>432.809165</td>\n",
       "      <td>108.445045</td>\n",
       "      <td>22.320676</td>\n",
       "      <td>28.462364</td>\n",
       "      <td>2.476381</td>\n",
       "      <td>1.512926</td>\n",
       "      <td>0.075566</td>\n",
       "      <td>108.445045</td>\n",
       "      <td>22.320676</td>\n",
       "      <td>28.460815</td>\n",
       "      <td>15.704016</td>\n",
       "      <td>258.827959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>668.713513</td>\n",
       "      <td>582.908732</td>\n",
       "      <td>350.324365</td>\n",
       "      <td>46.076570</td>\n",
       "      <td>55.646490</td>\n",
       "      <td>14.028052</td>\n",
       "      <td>28.527704</td>\n",
       "      <td>5.310951</td>\n",
       "      <td>350.324365</td>\n",
       "      <td>46.076570</td>\n",
       "      <td>55.645372</td>\n",
       "      <td>69.434167</td>\n",
       "      <td>452.892203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>50.730000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>46.670000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>250.000000</td>\n",
       "      <td>250.000000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>125.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>300.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>31467.400000</td>\n",
       "      <td>6600.000000</td>\n",
       "      <td>33333.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>1950.000000</td>\n",
       "      <td>600.000000</td>\n",
       "      <td>33333.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1500.000000</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>33333.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               c.77         c.125  ...        t7.157        t7.158\n",
       "count  18301.000000  18302.000000  ...  18376.000000  18303.000000\n",
       "mean     442.001570    432.809165  ...     15.704016    258.827959\n",
       "std      668.713513    582.908732  ...     69.434167    452.892203\n",
       "min        0.000000      0.000000  ...      0.000000      0.000000\n",
       "25%       50.730000     50.000000  ...      0.000000     46.670000\n",
       "50%      250.000000    250.000000  ...      0.000000    150.000000\n",
       "75%      500.000000    500.000000  ...      0.000000    300.000000\n",
       "max    31467.400000   6600.000000  ...   3000.000000  33333.000000\n",
       "\n",
       "[8 rows x 13 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[appending_columns].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C-l65hLByDR_",
    "outputId": "5f4e3197-e693-4f5d-d868-ef59316b4964"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total number of NAN values in current Dataset:  384\n",
      " Total number of NAN valuesin Dataset after filling:  0\n"
     ]
    }
   ],
   "source": [
    "print(' Total number of NAN values in current Dataset: ',data[appending_columns].isnull().sum().sum())\n",
    "\n",
    "#Filling all the NAN Values with the most frequent occuring values in the columns\n",
    "for column in data[appending_columns].columns:\n",
    "    data[column].fillna(data[column].mode()[0], inplace=True)\n",
    "\n",
    "print(' Total number of NAN valuesin Dataset after filling: ',data[appending_columns].isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i1KWWIgg00bS",
    "outputId": "986bdc04-6480-45d8-d701-a8a47eb5e005"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training Dataset: (18379, 241)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of Training Dataset:', data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sztbIWmJUKn2",
    "outputId": "4dddac82-0e79-4a75-cc1f-876578265b0e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 18379 entries, 0 to 18378\n",
      "Columns: 241 entries, Unnamed: 0 to c.283\n",
      "dtypes: float64(75), int64(165), object(1)\n",
      "memory usage: 33.8+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TOYPyPzGUxL9",
    "outputId": "ccf34d25-c813-40b7-eadc-7e9a6883caf5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "respondent.id    object\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "object_columns = data.dtypes[data.dtypes == np.object]\n",
    "print(object_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_1tCgT6WU_E5"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "data['respondent.id'] = data['respondent.id'].apply(lambda x : re.sub('2016_' , '' , x))\n",
    "data['respondent.id'] = data['respondent.id'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3b4v2KDWXB0Z",
    "outputId": "855ae37d-0455-4cab-f5a0-eebb2fb0e54f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], dtype: object)\n"
     ]
    }
   ],
   "source": [
    "object_columns = data.dtypes[data.dtypes == np.object]\n",
    "print(object_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 263
    },
    "id": "p-ziXuZyYo8N",
    "outputId": "8723470c-9c1a-4822-fb3c-fe2a40ea8a42"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>month</th>\n",
       "      <th>var8</th>\n",
       "      <th>var6</th>\n",
       "      <th>a.1</th>\n",
       "      <th>a.2</th>\n",
       "      <th>a.3</th>\n",
       "      <th>a.4</th>\n",
       "      <th>var5</th>\n",
       "      <th>b.5</th>\n",
       "      <th>b.6</th>\n",
       "      <th>b.7</th>\n",
       "      <th>b.8</th>\n",
       "      <th>b.9</th>\n",
       "      <th>b.10</th>\n",
       "      <th>b.11</th>\n",
       "      <th>b.12</th>\n",
       "      <th>b.13</th>\n",
       "      <th>b.14</th>\n",
       "      <th>b.15</th>\n",
       "      <th>b.16</th>\n",
       "      <th>b.17</th>\n",
       "      <th>b.18</th>\n",
       "      <th>b.19</th>\n",
       "      <th>b.20</th>\n",
       "      <th>b.21</th>\n",
       "      <th>b.22</th>\n",
       "      <th>pov6</th>\n",
       "      <th>b.23</th>\n",
       "      <th>b.24</th>\n",
       "      <th>b.25</th>\n",
       "      <th>b.26</th>\n",
       "      <th>b.27</th>\n",
       "      <th>b.28</th>\n",
       "      <th>c.29</th>\n",
       "      <th>c.30</th>\n",
       "      <th>var7</th>\n",
       "      <th>c.31</th>\n",
       "      <th>c.32</th>\n",
       "      <th>c.33</th>\n",
       "      <th>...</th>\n",
       "      <th>c.245</th>\n",
       "      <th>c.246</th>\n",
       "      <th>c.247</th>\n",
       "      <th>c.248</th>\n",
       "      <th>c.249</th>\n",
       "      <th>c.250</th>\n",
       "      <th>c.251</th>\n",
       "      <th>c.252</th>\n",
       "      <th>c.253</th>\n",
       "      <th>c.254</th>\n",
       "      <th>c.255</th>\n",
       "      <th>c.256</th>\n",
       "      <th>c.257</th>\n",
       "      <th>c.258</th>\n",
       "      <th>c.259</th>\n",
       "      <th>c.260</th>\n",
       "      <th>c.261</th>\n",
       "      <th>c.262</th>\n",
       "      <th>c.263</th>\n",
       "      <th>c.264</th>\n",
       "      <th>c.265</th>\n",
       "      <th>c.266</th>\n",
       "      <th>c.267</th>\n",
       "      <th>c.268</th>\n",
       "      <th>c.269</th>\n",
       "      <th>c.270</th>\n",
       "      <th>c.271</th>\n",
       "      <th>c.272</th>\n",
       "      <th>c.273</th>\n",
       "      <th>c.274</th>\n",
       "      <th>c.275</th>\n",
       "      <th>c.276</th>\n",
       "      <th>c.277</th>\n",
       "      <th>c.278</th>\n",
       "      <th>c.279</th>\n",
       "      <th>c.280</th>\n",
       "      <th>c.281</th>\n",
       "      <th>c.282</th>\n",
       "      <th>c.283</th>\n",
       "      <th>f.284</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>47</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>211.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>211.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>31</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56</td>\n",
       "      <td>56</td>\n",
       "      <td>56.0</td>\n",
       "      <td>56.0</td>\n",
       "      <td>12</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>45</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>90.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>83</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>94.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>200.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 300 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  month  var8  var6  a.1  ...  c.280  c.281  c.282  c.283  f.284\n",
       "0           9      1   1.0   NaN    1  ...      0      0      1      0    3.0\n",
       "1          15      1   2.0   NaN    1  ...      0      0      0      0    3.0\n",
       "2          16      1   2.0   NaN    1  ...      0      0      1      0    3.0\n",
       "3          24      1   1.0   NaN    1  ...      0      0      0      0    NaN\n",
       "4          32      1   1.0   NaN    1  ...      1      0      0      0    NaN\n",
       "\n",
       "[5 rows x 300 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Reading the test data in tabular formal using pandas\n",
    "test_df = pd.read_csv('/content/drive/MyDrive/TestData.csv')\n",
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-weCTQTO1ot2"
   },
   "source": [
    "Since we are traning our dataset with reduced number of columns/Features , hence we need to remove those columns/features from test data as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "id": "ERolR8JK12xl",
    "outputId": "0a7726e4-2643-43d6-a7d6-1a6c0b28b972"
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-176-7cf508b73e95>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mrejected_columns\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Shape of Testing Data: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4172\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4173\u001b[0m             \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4174\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4175\u001b[0m         )\n\u001b[1;32m   4176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3887\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3888\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3889\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3890\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3891\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3921\u001b[0m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3922\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3923\u001b[0;31m                 \u001b[0mnew_axis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3924\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0maxis_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnew_axis\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   5285\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5286\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5287\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels[mask]} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5288\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5289\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelete\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['t.158'] not found in axis\""
     ]
    }
   ],
   "source": [
    "test_df.drop(columns= rejected_columns , inplace = True )\n",
    "print('Shape of Testing Data: ', test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFDyp2RI2RoJ"
   },
   "source": [
    "It seems column/feature : t.158 is not present in test_data so either we need to populate this feild in testing data or we need to remove it entirely from the training data as well\n",
    "\n",
    "We have already removed the column/Feature t.158 from the training data since it had null values in three digits in earlier part of Preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MEMjIu8x2Odg"
   },
   "outputs": [],
   "source": [
    "#Here we are removing the column t.158 from the rejected_column list using remove \n",
    "rejected_columns.remove('t.158')\n",
    "test_df.drop(columns= rejected_columns , inplace = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ni7v9LLFZuwH",
    "outputId": "fb7b058f-ac59-4b38-b3cf-0c0d9f89df30"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Testing Data:  (4595, 241)\n"
     ]
    }
   ],
   "source": [
    "# Number of cloumns/Features in Testing data should be same as that of Training Data\n",
    "print('Shape of Testing Data: ',test_df.shape)\n",
    "\n",
    "#Filling all the NAN Values with the most frequent occuring values in the columns\n",
    "for column in test_df[appending_columns].columns:\n",
    "    test_df[column].fillna(test_df[column].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KhvEnNvwcT-8"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "test_df['respondent.id'] = test_df['respondent.id'].apply(lambda x : re.sub('2016_' , '' , x))\n",
    "test_df['respondent.id'] = test_df['respondent.id'].astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cZb0HPlRaKpW",
    "outputId": "7fb413d0-9f38-4b24-e626-dc56e901c807"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Traning Data:  (12313, 240)\n",
      "shape of Cross-validation Data:  (6066, 240)\n",
      "Shape of Testing Data:  (4595, 240)\n"
     ]
    }
   ],
   "source": [
    "#Now we would split our training , testing and Cross_validation data\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = data.drop(columns= ['pov6'] )\n",
    "y = data['pov6']\n",
    "\n",
    "x_train , x_cv , y_train , y_cv = train_test_split(x,y , test_size = 0.33 , random_state = 42)\n",
    "x_test = test_df.drop(columns= ['pov6'])\n",
    "y_test = test_df['pov6']\n",
    "\n",
    "print('Shape of Traning Data: ', x_train.shape)\n",
    "print('shape of Cross-validation Data: ', x_cv.shape)\n",
    "print('Shape of Testing Data: ', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ruW1INPx7ZcX",
    "outputId": "72d783d8-8197-41ee-f3dd-31d5ed6143cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training Class label Data:  (12313, 6)\n",
      "Shape of Cross-Validation Class label Data:  (6066, 6)\n"
     ]
    }
   ],
   "source": [
    "# We will one hot encode the Class label column in order to do Multi-class classification Problem\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "le = LabelEncoder()\n",
    "#Traning Class labels\n",
    "y_train = le.fit_transform(y_train)\n",
    "y_train = to_categorical(y_train)\n",
    "print('Shape of Training Class label Data: ', y_train.shape)\n",
    "#Cross-validation Class labels\n",
    "y_cv = le.transform(y_cv)\n",
    "y_cv = to_categorical(y_cv)\n",
    "print('Shape of Cross-Validation Class label Data: ', y_cv.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cOoJSjt39BQG",
    "outputId": "98e7bed3-44a1-4e22-bb0d-a8278b0d7b5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null_values in Testing_Class Labels:  4595\n"
     ]
    }
   ],
   "source": [
    "print('Null_values in Testing_Class Labels: ',y_test.isnull().sum().sum())\n",
    "\n",
    "#In testing Data we don't have any class labels and we would need to predict all of them using your model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZJ6RHHdi5YCZ"
   },
   "source": [
    "a. Please explain the choice of metric / evaluation criterion used :\n",
    "\n",
    "**Metric:** Since we have a lot of class imbalance thus we cannot use accuracy as a metric for this problem statement and hence we need to choose a metric which could provide the performance each class wise and then weighted average of that performance would be our metric. This could be achived using F1_score which uses precision and recall class wise in multi-class problems.\n",
    "\n",
    "A macro-average will compute the metric independently for each class and then take the average and hence in the process it treats all the classes equally , whereas a micro-average will aggregate the contributions of all classes to compute the average metric.\n",
    "Thus we would use Micro F1_score to compute our model's performance \n",
    "</br>\n",
    "</br>\n",
    "**Model:** Here we are choosing one of the ensemble's model RandomForest since it has multiple estimators and would work great in case of outliers and will keep an eye on the metric through F1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l601rG3xcBiC"
   },
   "source": [
    "What were the approaches you considered? Please explain the reason for the technique/ approach used as well as the pros and cons.\n",
    "\n",
    "Here in this task I have used Hyper-parameter tunning in order to find the best possible value of F1_score through RandomizedSearchCV.\n",
    "Also the training data is divided into 2 parts using sklearn's train_test_split : one part is used to train our model while the other is kept unseen to the model and used for generating cross validation results to observe the performance of the model. Since there is a class imbalance and entries for group 6,5 and 4 are way less , we might have end up traning over model with very less entires which belongs to these groups "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7xlQxAKVBCqO",
    "outputId": "5cdd963b-756d-4e1c-f49f-f2a19747cba6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
      "[CV] n_estimators=1000, min_samples_split=4 ..........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=1000, min_samples_split=4, score=(train=0.994, test=0.948), total=  21.7s\n",
      "[CV] n_estimators=1000, min_samples_split=4 ..........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   24.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=1000, min_samples_split=4, score=(train=0.994, test=0.951), total=  22.5s\n",
      "[CV] n_estimators=1000, min_samples_split=4 ..........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:   50.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=1000, min_samples_split=4, score=(train=0.995, test=0.949), total=  22.3s\n",
      "[CV] n_estimators=2000, min_samples_split=3 ..........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  1.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=2000, min_samples_split=3, score=(train=1.000, test=0.947), total=  43.6s\n",
      "[CV] n_estimators=2000, min_samples_split=3 ..........................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   4 out of   4 | elapsed:  2.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=2000, min_samples_split=3, score=(train=0.999, test=0.951), total=  45.6s\n",
      "[CV] n_estimators=2000, min_samples_split=3 ..........................\n",
      "[CV]  n_estimators=2000, min_samples_split=3, score=(train=1.000, test=0.949), total=  44.3s\n",
      "[CV] n_estimators=2000, min_samples_split=2 ..........................\n",
      "[CV]  n_estimators=2000, min_samples_split=2, score=(train=1.000, test=0.948), total=  44.4s\n",
      "[CV] n_estimators=2000, min_samples_split=2 ..........................\n",
      "[CV]  n_estimators=2000, min_samples_split=2, score=(train=1.000, test=0.951), total=  45.7s\n",
      "[CV] n_estimators=2000, min_samples_split=2 ..........................\n",
      "[CV]  n_estimators=2000, min_samples_split=2, score=(train=1.000, test=0.950), total=  44.6s\n",
      "[CV] n_estimators=2000, min_samples_split=4 ..........................\n",
      "[CV]  n_estimators=2000, min_samples_split=4, score=(train=0.994, test=0.948), total=  43.7s\n",
      "[CV] n_estimators=2000, min_samples_split=4 ..........................\n",
      "[CV]  n_estimators=2000, min_samples_split=4, score=(train=0.994, test=0.950), total=  45.6s\n",
      "[CV] n_estimators=2000, min_samples_split=4 ..........................\n",
      "[CV]  n_estimators=2000, min_samples_split=4, score=(train=0.995, test=0.949), total=  44.1s\n",
      "[CV] n_estimators=500, min_samples_split=3 ...........................\n",
      "[CV]  n_estimators=500, min_samples_split=3, score=(train=0.999, test=0.948), total=  10.8s\n",
      "[CV] n_estimators=500, min_samples_split=3 ...........................\n",
      "[CV]  n_estimators=500, min_samples_split=3, score=(train=0.999, test=0.952), total=  11.4s\n",
      "[CV] n_estimators=500, min_samples_split=3 ...........................\n",
      "[CV]  n_estimators=500, min_samples_split=3, score=(train=0.999, test=0.948), total=  11.3s\n",
      "[CV] n_estimators=500, min_samples_split=1 ...........................\n",
      "[CV]  n_estimators=500, min_samples_split=1, score=(train=nan, test=nan), total=   0.2s\n",
      "[CV] n_estimators=500, min_samples_split=1 ...........................\n",
      "[CV]  n_estimators=500, min_samples_split=1, score=(train=nan, test=nan), total=   0.2s\n",
      "[CV] n_estimators=500, min_samples_split=1 ...........................\n",
      "[CV]  n_estimators=500, min_samples_split=1, score=(train=nan, test=nan), total=   0.2s\n",
      "[CV] n_estimators=500, min_samples_split=2 ...........................\n",
      "[CV]  n_estimators=500, min_samples_split=2, score=(train=1.000, test=0.947), total=  11.2s\n",
      "[CV] n_estimators=500, min_samples_split=2 ...........................\n",
      "[CV]  n_estimators=500, min_samples_split=2, score=(train=1.000, test=0.952), total=  11.4s\n",
      "[CV] n_estimators=500, min_samples_split=2 ...........................\n",
      "[CV]  n_estimators=500, min_samples_split=2, score=(train=1.000, test=0.948), total=  11.2s\n",
      "[CV] n_estimators=2000, min_samples_split=1 ..........................\n",
      "[CV]  n_estimators=2000, min_samples_split=1, score=(train=nan, test=nan), total=   0.7s\n",
      "[CV] n_estimators=2000, min_samples_split=1 ..........................\n",
      "[CV]  n_estimators=2000, min_samples_split=1, score=(train=nan, test=nan), total=   0.6s\n",
      "[CV] n_estimators=2000, min_samples_split=1 ..........................\n",
      "[CV]  n_estimators=2000, min_samples_split=1, score=(train=nan, test=nan), total=   0.6s\n",
      "[CV] n_estimators=100, min_samples_split=2 ...........................\n",
      "[CV]  n_estimators=100, min_samples_split=2, score=(train=1.000, test=0.947), total=   2.3s\n",
      "[CV] n_estimators=100, min_samples_split=2 ...........................\n",
      "[CV]  n_estimators=100, min_samples_split=2, score=(train=1.000, test=0.950), total=   2.3s\n",
      "[CV] n_estimators=100, min_samples_split=2 ...........................\n",
      "[CV]  n_estimators=100, min_samples_split=2, score=(train=1.000, test=0.947), total=   2.3s\n",
      "[CV] n_estimators=1000, min_samples_split=1 ..........................\n",
      "[CV]  n_estimators=1000, min_samples_split=1, score=(train=nan, test=nan), total=   0.3s\n",
      "[CV] n_estimators=1000, min_samples_split=1 ..........................\n",
      "[CV]  n_estimators=1000, min_samples_split=1, score=(train=nan, test=nan), total=   0.3s\n",
      "[CV] n_estimators=1000, min_samples_split=1 ..........................\n",
      "[CV]  n_estimators=1000, min_samples_split=1, score=(train=nan, test=nan), total=   0.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  30 out of  30 | elapsed: 10.3min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3, error_score=nan,\n",
       "                   estimator=RandomForestClassifier(bootstrap=True,\n",
       "                                                    ccp_alpha=0.0,\n",
       "                                                    class_weight=None,\n",
       "                                                    criterion='gini',\n",
       "                                                    max_depth=None,\n",
       "                                                    max_features='auto',\n",
       "                                                    max_leaf_nodes=None,\n",
       "                                                    max_samples=None,\n",
       "                                                    min_impurity_decrease=0.0,\n",
       "                                                    min_impurity_split=None,\n",
       "                                                    min_samples_leaf=1,\n",
       "                                                    min_samples_split=2,\n",
       "                                                    min_weight_fraction_leaf=0.0,\n",
       "                                                    n_estimators=100,\n",
       "                                                    n_jobs=None,\n",
       "                                                    oob_score=False,\n",
       "                                                    random_state=None,\n",
       "                                                    verbose=0,\n",
       "                                                    warm_start=False),\n",
       "                   iid='deprecated', n_iter=10, n_jobs=None,\n",
       "                   param_distributions={'min_samples_split': [1, 2, 3, 4],\n",
       "                                        'n_estimators': [100, 300, 500, 1000,\n",
       "                                                         2000]},\n",
       "                   pre_dispatch='2*n_jobs', random_state=None, refit=True,\n",
       "                   return_train_score=True,\n",
       "                   scoring=make_scorer(f1_score, average=micro), verbose=5)"
      ]
     },
     "execution_count": 44,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparamter tunning the essential parameters of the RandomForest Classifier\n",
    "# Using RamdomizedSearchCV for Hypertunning \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import f1_score, make_scorer\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# making a f1_score funtion separetly in order to fit that into RandomizedSearchCV\n",
    "f1 = make_scorer(f1_score , average = 'micro')\n",
    "model = RandomForestClassifier()\n",
    "\n",
    "#parameters considered for hyperparameter tunning are min_samples_split and n_estimators\n",
    "parameters = {'min_samples_split' :[1,2,3,4],\n",
    "              'n_estimators' : [100,300,500,1000,2000]}\n",
    "\n",
    "cross_validation = RandomizedSearchCV(model, parameters, cv = 3 , scoring = f1 , return_train_score= True, verbose = 5)\n",
    "cross_validation.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 867
    },
    "id": "IDdubMM5Hl7W",
    "outputId": "ea280c05-a4fa-4ef9-bb9b-d88cbd06e899"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>param_min_samples_split</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>20.575007</td>\n",
       "      <td>0.317045</td>\n",
       "      <td>1.571967</td>\n",
       "      <td>0.010299</td>\n",
       "      <td>1000</td>\n",
       "      <td>4</td>\n",
       "      <td>{'n_estimators': 1000, 'min_samples_split': 4}</td>\n",
       "      <td>0.948187</td>\n",
       "      <td>0.950771</td>\n",
       "      <td>0.948848</td>\n",
       "      <td>0.949268</td>\n",
       "      <td>0.001096</td>\n",
       "      <td>3</td>\n",
       "      <td>0.994138</td>\n",
       "      <td>0.993894</td>\n",
       "      <td>0.994626</td>\n",
       "      <td>0.994219</td>\n",
       "      <td>0.000305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41.368952</td>\n",
       "      <td>0.792065</td>\n",
       "      <td>3.136507</td>\n",
       "      <td>0.009203</td>\n",
       "      <td>2000</td>\n",
       "      <td>3</td>\n",
       "      <td>{'n_estimators': 2000, 'min_samples_split': 3}</td>\n",
       "      <td>0.947109</td>\n",
       "      <td>0.951277</td>\n",
       "      <td>0.948885</td>\n",
       "      <td>0.949090</td>\n",
       "      <td>0.001708</td>\n",
       "      <td>5</td>\n",
       "      <td>0.999574</td>\n",
       "      <td>0.999452</td>\n",
       "      <td>0.999695</td>\n",
       "      <td>0.999574</td>\n",
       "      <td>0.000099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41.669513</td>\n",
       "      <td>0.536298</td>\n",
       "      <td>3.184189</td>\n",
       "      <td>0.038965</td>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>{'n_estimators': 2000, 'min_samples_split': 2}</td>\n",
       "      <td>0.947706</td>\n",
       "      <td>0.951499</td>\n",
       "      <td>0.949704</td>\n",
       "      <td>0.949637</td>\n",
       "      <td>0.001549</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>41.303706</td>\n",
       "      <td>0.787145</td>\n",
       "      <td>3.164752</td>\n",
       "      <td>0.025925</td>\n",
       "      <td>2000</td>\n",
       "      <td>4</td>\n",
       "      <td>{'n_estimators': 2000, 'min_samples_split': 4}</td>\n",
       "      <td>0.947927</td>\n",
       "      <td>0.950278</td>\n",
       "      <td>0.948885</td>\n",
       "      <td>0.949030</td>\n",
       "      <td>0.000965</td>\n",
       "      <td>6</td>\n",
       "      <td>0.994321</td>\n",
       "      <td>0.994323</td>\n",
       "      <td>0.995176</td>\n",
       "      <td>0.994607</td>\n",
       "      <td>0.000403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10.394404</td>\n",
       "      <td>0.257945</td>\n",
       "      <td>0.800420</td>\n",
       "      <td>0.008166</td>\n",
       "      <td>500</td>\n",
       "      <td>3</td>\n",
       "      <td>{'n_estimators': 500, 'min_samples_split': 3}</td>\n",
       "      <td>0.948238</td>\n",
       "      <td>0.951923</td>\n",
       "      <td>0.948238</td>\n",
       "      <td>0.949466</td>\n",
       "      <td>0.001737</td>\n",
       "      <td>2</td>\n",
       "      <td>0.999330</td>\n",
       "      <td>0.999330</td>\n",
       "      <td>0.999452</td>\n",
       "      <td>0.999370</td>\n",
       "      <td>0.000057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.174566</td>\n",
       "      <td>0.004493</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>500</td>\n",
       "      <td>1</td>\n",
       "      <td>{'n_estimators': 500, 'min_samples_split': 1}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10.473683</td>\n",
       "      <td>0.113159</td>\n",
       "      <td>0.799336</td>\n",
       "      <td>0.002848</td>\n",
       "      <td>500</td>\n",
       "      <td>2</td>\n",
       "      <td>{'n_estimators': 500, 'min_samples_split': 2}</td>\n",
       "      <td>0.947433</td>\n",
       "      <td>0.951511</td>\n",
       "      <td>0.948497</td>\n",
       "      <td>0.949147</td>\n",
       "      <td>0.001727</td>\n",
       "      <td>4</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.638818</td>\n",
       "      <td>0.011814</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2000</td>\n",
       "      <td>1</td>\n",
       "      <td>{'n_estimators': 2000, 'min_samples_split': 1}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.123621</td>\n",
       "      <td>0.024988</td>\n",
       "      <td>0.170614</td>\n",
       "      <td>0.002941</td>\n",
       "      <td>100</td>\n",
       "      <td>2</td>\n",
       "      <td>{'n_estimators': 100, 'min_samples_split': 2}</td>\n",
       "      <td>0.947057</td>\n",
       "      <td>0.949525</td>\n",
       "      <td>0.947304</td>\n",
       "      <td>0.947962</td>\n",
       "      <td>0.001110</td>\n",
       "      <td>7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.327094</td>\n",
       "      <td>0.005693</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>{'n_estimators': 1000, 'min_samples_split': 1}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  ...  mean_train_score  std_train_score\n",
       "0      20.575007      0.317045  ...          0.994219         0.000305\n",
       "1      41.368952      0.792065  ...          0.999574         0.000099\n",
       "2      41.669513      0.536298  ...          1.000000         0.000000\n",
       "3      41.303706      0.787145  ...          0.994607         0.000403\n",
       "4      10.394404      0.257945  ...          0.999370         0.000057\n",
       "5       0.174566      0.004493  ...               NaN              NaN\n",
       "6      10.473683      0.113159  ...          1.000000         0.000000\n",
       "7       0.638818      0.011814  ...               NaN              NaN\n",
       "8       2.123621      0.024988  ...          1.000000         0.000000\n",
       "9       0.327094      0.005693  ...               NaN              NaN\n",
       "\n",
       "[10 rows x 18 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = pd.DataFrame.from_dict(cross_validation.cv_results_)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "id": "sBar7CRfKEDi",
    "outputId": "dbbc5ae3-7c00-42ff-8382-5c6a511763b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Hyperparameter Tunning Test Scores:F1_score')"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Creating a Heatmap for Visualizing F1-score using different values of parameters \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "df = pd.DataFrame()\n",
    "df['n_estimators'] = results['param_n_estimators']\n",
    "df['min_samples_split'] = results['param_min_samples_split']\n",
    "df['train_score'] = results['mean_train_score']\n",
    "df['test_score'] = results['mean_test_score']\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(1,2,1)\n",
    "heatmap_data1 = pd.pivot_table(df, values = 'train_score' , index = ['n_estimators'] , columns = 'min_samples_split')\n",
    "sns.heatmap(heatmap_data1, annot = True , cmap=\"viridis\")\n",
    "plt.title('Hyperparameter Tunning Train Scores: F1_score')\n",
    "plt.subplot(1,2,2)\n",
    "heatmap_data2 = pd.pivot_table(df, values = 'test_score' , index = ['n_estimators'] , columns = 'min_samples_split')\n",
    "sns.heatmap(heatmap_data2, annot = True , cmap=\"viridis\")\n",
    "plt.title('Hyperparameter Tunning Test Scores:F1_score')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PW2PTb0jbTWi"
   },
   "source": [
    "b. What are the assumptions you made when building this model?\n",
    "\n",
    "The assumptions made while building our RandomForest Classifier are as follows:\n",
    "\n",
    "1.   At each step of building individual tree we find the best split of data\n",
    "2.   While building a tree we use not the whole dataset, but bootstrap sample\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uMafBXQAJ-3C",
    "outputId": "0c89bb04-ddbd-4261-882d-9d45f18ea140"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=2000,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using Best parameters claculated using Hyperparameter Tunning to train the RandomForest model\n",
    "Rf = RandomForestClassifier(n_estimators = cross_validation.best_params_.get('n_estimators')\n",
    "                            ,min_samples_split = cross_validation.best_params_.get('min_samples_split')  )\n",
    "Rf.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VVjS1ztwUPnG",
    "outputId": "f1889675-9612-44e7-bedb-46f51e8065cd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of predicted value for Cross_valiidation:  (6066, 6)\n",
      "Cross-Validation _Results:\n",
      "\n",
      "Confusion Matrix:\n",
      "[[4633   34    0    1    0    0]\n",
      " [ 132  939    0    0    0    0]\n",
      " [  20    4   89    0    0    0]\n",
      " [  34   22    0    8    0    2]\n",
      " [  27    3    0    0   16    0]\n",
      " [  43    9    0    0    0   50]]\n",
      "\n",
      "Classification_Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98      4668\n",
      "           1       0.93      0.88      0.90      1071\n",
      "           2       1.00      0.79      0.88       113\n",
      "           3       0.89      0.12      0.21        66\n",
      "           4       1.00      0.35      0.52        46\n",
      "           5       0.96      0.49      0.65       102\n",
      "\n",
      "   micro avg       0.96      0.94      0.95      6066\n",
      "   macro avg       0.96      0.60      0.69      6066\n",
      "weighted avg       0.96      0.94      0.94      6066\n",
      " samples avg       0.94      0.94      0.94      6066\n",
      "\n",
      "F1-Score 0.95077590522276\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "\n",
    "#Predicting the class labels for cross-validation report \n",
    "y_pred = Rf.predict(x_cv)\n",
    "print('Shape of predicted value for Cross_valiidation: ',y_pred.shape)\n",
    "\n",
    "print('Cross-Validation _Results:\\n')\n",
    "matrix = confusion_matrix(y_cv.argmax(axis=1), y_pred.argmax(axis=1))\n",
    "print('Confusion Matrix:')\n",
    "print(matrix)\n",
    "\n",
    "print('\\nClassification_Report:')\n",
    "print(classification_report(y_cv, y_pred))\n",
    "print('F1-Score', f1_score(y_cv, y_pred , average= 'micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XCBXoy31dxFP"
   },
   "source": [
    "e. How confident are you of the modelâ€™s robustness and how would you explain the modelâ€™s performance?\n",
    "\n",
    "The randomness in data whcih is used to train the Decision Trees makes the RandomForests robust to outliers to some extent.\n",
    "The metric we used to evaluate our model i.e. Micro F1_score comes out to be 0.95 which is a decent value and thus we conclude that our model is performing well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "InFTOZz0gQ6k"
   },
   "source": [
    "f. Why is your model performing well / not well?\n",
    "\n",
    "RandomForest Classifier works well tabular kind of data and also the randomness in selecting samples from the training data for training different Decision Trees to its max depth adds a plus point which helps the model to perform well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "n30hJ_XKVsAp",
    "outputId": "dbf1947d-dfb3-46c0-9291-96a65d5d297b"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-212-80c9abab7498>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    610\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mpredicted\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \"\"\"\n\u001b[0;32m--> 612\u001b[0;31m         \u001b[0mproba\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    654\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m         \u001b[0;31m# Check data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m         \u001b[0;31m# Assign chunk of trees to jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    410\u001b[0m         \u001b[0mcheck_is_fitted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 412\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    413\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[0;34m(self, X, check_input)\u001b[0m\n\u001b[1;32m    378\u001b[0m         \u001b[0;34m\"\"\"Validate X whenever one tries to predict, apply, predict_proba\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"csr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m             if issparse(X) and (X.indices.dtype != np.intc or\n\u001b[1;32m    382\u001b[0m                                 X.indptr.dtype != np.intc):\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    576\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 578\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m                     (type_err,\n\u001b[0;32m---> 60\u001b[0;31m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[1;32m     61\u001b[0m             )\n\u001b[1;32m     62\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "y_test = Rf.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7V0RvV3XRGC"
   },
   "source": [
    "It seems our Test Data conatains some NAN values and we would have to observe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i9hLcnOhXGj4",
    "outputId": "336afd5f-e331-4e2c-94f8-4d445b77ba80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "totshopping.rep Column/Feature total number of NAN Values: 4595\n"
     ]
    }
   ],
   "source": [
    "data_columns = list(x_test.columns)\n",
    "null_columns = []\n",
    "non_null_columns = []\n",
    "null_values = []\n",
    "count = 0\n",
    "for i in range(len(data_columns)):\n",
    "  if x_test[data_columns[i]].isnull().sum() == 0:\n",
    "    non_null_columns.append(data_columns[i])\n",
    "    pass\n",
    "  else:\n",
    "    print(data_columns[i], end = \" \")\n",
    "    print('Column/Feature total number of NAN Values:', end= ' ')\n",
    "    print(x_test[data_columns[i]].isnull().sum())\n",
    "    count += 1\n",
    "    null_columns.append(data_columns[i])\n",
    "    null_values.append(x_test[data_columns[i]].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zqajLdjVX4fK"
   },
   "source": [
    "This means that the entire Column/Feature totshopping.rep conatins NULL values which is generating such problem while predicting the class labels and this a prblem of Data Leakage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jP_wYYu-X3pc",
    "outputId": "f2570737-e06c-438e-9cec-b363a60131ac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    12313.000000\n",
       "mean       331.300123\n",
       "std        366.590573\n",
       "min          1.620000\n",
       "25%        114.060000\n",
       "50%        228.600000\n",
       "75%        420.890000\n",
       "max       5078.540000\n",
       "Name: totshopping.rep, dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train['totshopping.rep'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPQhuTjhD8ks"
   },
   "source": [
    "Here we have two options either to remove the entire Column/Featrue or to populate the entire column using some methodlogy.\n",
    "\n",
    "We weould try the 2nd option and try to populate the values for this Column/Featrue using a Linear Regression Model and will use RSquared Coefficient in order to check how well is it populating in the field "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_hD8eug5YskR",
    "outputId": "25320543-5f2a-4cb1-e5db-ce6119516f2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data shape:  (8249, 239)\n",
      "Cross_validation Data shape:  (4064, 239)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_shopping = x_train.drop(columns = ['totshopping.rep'])\n",
    "y_shopping = x_train['totshopping.rep']\n",
    "\n",
    "x_shopping_train, x_shopping_cv , y_shopping_train , y_shopping_cv = train_test_split(x_shopping, y_shopping , test_size = 0.33)\n",
    "\n",
    "print('Training Data shape: ', x_shopping_train.shape)\n",
    "print('Cross_validation Data shape: ', x_shopping_cv.shape)\n",
    "\n",
    "#Import Linear Regression Model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "#Creating an Object of Linear Regression\n",
    "lr = LinearRegression()\n",
    "lr.fit(x_shopping_train, y_shopping_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6kJ3LUZfaONN",
    "outputId": "f8fda20a-137b-409c-c7c1-aad8b8c80129"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of CV Data :  (4064,)\n",
      "Shape of Predicted Data:  (4064,)\n",
      "Cross-validation Metric for Linear Regression:  0.999999102310064\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "print('shape of CV Data : ', y_shopping_cv.shape)\n",
    "y_pred = lr.predict(x_shopping_cv)\n",
    "print('Shape of Predicted Data: ',y_pred.shape)\n",
    "\n",
    "print('Cross-validation Metric for Linear Regression: ',r2_score(y_shopping_cv, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "90_EW8-5F9Gv"
   },
   "source": [
    "g. Was any feature engineering required? If yes, what were they. If no, why?\n",
    "\n",
    "Performance of Linear Regression Model is quite good and I guess we could use this model in order populate the Column: 'totshopping.rep'\n",
    "using this model.\n",
    "</br>\n",
    "This entire process in known as Model based Imputation \n",
    "\n",
    "This was the only Feature Engineering required since the Feature : 'totshopping.rep' had only Null values and we needed to do Model based Impuatation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m8dXCMsYbCrh",
    "outputId": "3c0a9dd3-730e-43aa-af72-fd3b7cd88452"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Prediction:  (4595,)\n"
     ]
    }
   ],
   "source": [
    "#Predicting Column 'totshopping.rep' in test data in order to help the prediction of groups using RandomForests Model\n",
    "pred = lr.predict(x_test.drop(columns= ['totshopping.rep']))\n",
    "print('Shape of Prediction: ',pred.shape)\n",
    "\n",
    "x_test['totshopping.rep'] = pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oalaDxPobtoz",
    "outputId": "ed1ce10f-c0d7-44e5-e769-1d627f993395"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Prediction of respondents group in pov6:  (4595, 6)\n",
      "Shape of array conatining Group Predictions:  (4595,)\n",
      "\n",
      "Number of Groups present in Test Data: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    3685\n",
       "2     790\n",
       "3      71\n",
       "6      32\n",
       "5      15\n",
       "4       2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 54,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "y_test = Rf.predict(x_test)\n",
    "print('Shape of Prediction of respondents group in pov6: ',y_test.shape)\n",
    "\n",
    "group_predictions = np.argmax(y_test, axis= 1)\n",
    "print('Shape of array conatining Group Predictions: ',group_predictions.shape)\n",
    "\n",
    "for i in range(len(group_predictions)):\n",
    "  group_predictions[i] = group_predictions[i]+1\n",
    "\n",
    "group_predictions = pd.DataFrame(group_predictions)\n",
    "print('\\nNumber of Groups present in Test Data: ')\n",
    "group_predictions.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vUPC7OIjqKtM",
    "outputId": "0a544c4d-b23b-4346-d9a8-3b5d5c70c70c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RandomForest_Model.pkl']"
      ]
     },
     "execution_count": 56,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Dumping the model in order to use it afterwards to predict the testing data\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "joblib.dump(Rf , 'RandomForest_Model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8qqOcBtLL1m"
   },
   "source": [
    "d. Please explain under what conditions will the model you choose be not appropriate\n",
    "\n",
    "If the Dataset had been too sparse then our model RandomForest wouldn't have worked as well as it is working right now \n",
    "\n",
    "If the Data used to train would be image data or graph data even then our model wouldn't have worked this properly \n",
    "\n",
    "If the test_data has all together different kind of data distribution then predicted results might not be that accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "la9JrMgwh7bM"
   },
   "source": [
    "# Future Work:\n",
    "\n",
    "1. Further we could use feature imporatnce in order to get the features which contributes maximum in the performance of model. These Features could be transformed basically know as Feature Tranform which takes the features into a higher dimensional space and new features could be added up to the list which might help in increasing performance of model.\n",
    "<br>\n",
    "Note: Decision Tree based model donot require scaling.\n",
    "</br>\n",
    "\n",
    "\n",
    "2. We could use deep learning models on this dataset keeping activation functions as sigmoid since it would keep the effect of outliers less due to squashing. Also we would try scaling the data and apply relu activation functions but we need to keep in mind that vanishing gradient prblem don't occur due to high number to parameters and chain rle of differentiation.\n",
    "\n",
    "3. We can also use linear models such as SVM after standardizing the data along with its other kernel versions where we could use RBF , polynomial kernel in order to handle non-linearity in the data.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BlackBuckInsights_1",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
